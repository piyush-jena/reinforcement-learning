{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCn5gK-8Qhtz"
      },
      "source": [
        "#  Instruction\n",
        "\n",
        "In this notebook, we will learn how to implement DQN using Tensorflow for the [Cartpole environment in OpenAI gym](https://gymnasium.farama.org/environments/classic_control/cart_pole/). You are given a basic skeleton but you need to complete the code where appropriate to solve the cartpole problem.\n",
        "\n",
        "You are free to tweak the code at any part. Your are also free to tweak the hyper-parameters to improve the performance of the agent. At the end you have to evaluate the performance of the agent on 100 independent episodes on the environment and print out the average testing performance.\n",
        "\n",
        "Make sure that your final submission is a notebook that can be run from beginning to end, and that you print out the performance of the agent at the end of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-Vo-FNPRX9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29432e24-278f-4d0a-e962-610fbc6a6568"
      },
      "source": [
        "import os\n",
        "!{os.sys.executable} -m pip install gymnasium\n",
        "!{os.sys.executable} -m pip install Pillow\n",
        "!{os.sys.executable} -m pip install ipython\n",
        "!{os.sys.executable} -m pip install pygame\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.9/dist-packages (0.28.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (1.22.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (6.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (3.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (8.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.9/dist-packages (7.34.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/dist-packages (from ipython) (0.18.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython) (67.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython) (3.0.38)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython) (2.14.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from ipython) (0.1.6)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.9/dist-packages (2.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZNKbkOPTeLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "401288c7-2062-4bea-a1de-61decebd1e01"
      },
      "source": [
        "# Parameters\n",
        "gamma = 0.95  # discount \n",
        "envname = \"CartPole-v1\"  # environment name\n",
        "env=gym.make(envname, render_mode=\"rgb_array\")\n",
        "\n",
        "obssize = env.observation_space.low.size\n",
        "actsize = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GujIeXW2RulT"
      },
      "source": [
        "## DQN (Deep Q Network)\n",
        "\n",
        "In previous HWs, we have learned to use Tensorflow to build deep learning models. In this HW, we will apply deep learning as function approximations in reinforcement learning. \n",
        "\n",
        "Reference: DQN https://arxiv.org/abs/1312.5602\n",
        "\n",
        "\n",
        "In tabular Q-learning, we maintain a table of state-action pairs $(s,a)$ and save one action value for each entry $Q(s,a),\\forall (s,a)$. At each time step $t$, we are in state $s_t$, then we choose action based on $\\epsilon-$greedy strategy. With prob $\\epsilon$, choose action uniformly random; with prob $1-\\epsilon$, choose action based on $$a_t = \\arg\\max_a Q(s_t,a)$$ \n",
        "\n",
        "We then get the instant reward $r_t$, update the Q-table using the following rule\n",
        "\n",
        "$$Q(s_t,a_t) \\leftarrow (1-\\alpha)Q(s_t,a_t) + \\alpha (r_t + \\max_a \\gamma Q(s_{t+1},a))$$\n",
        "\n",
        "where $\\alpha \\in (0,1)$ is learning rate. The algorithm is shown to converge in tabular cases. However, in cases where we cannot keep a table for state and action, we need function approximation. Consider using neural network with parameter $\\theta$, the network takes as input state $s$ and action $a$. (*or some features of state and action*). Let $Q_\\theta(s,a)$ be the output of the network, which estimates the optimal Q-value function for state $s$ and action $a$.\n",
        "$$Q_\\theta(s,a) \\approx Q^\\ast(s,a)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "494xoDa8SLHG"
      },
      "source": [
        "def model_creator():\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(24,activation='relu', kernel_initializer=tf.keras.initializers.GlorotNormal))\n",
        "\n",
        "    model.add(layers.Dense(24,activation='relu', kernel_initializer=tf.keras.initializers.GlorotNormal))\n",
        "    # you should later modify this neural network\n",
        "    model.add(layers.Dense(actsize, kernel_initializer=tf.keras.initializers.GlorotNormal)) # you should have one output for each possible action\n",
        "    return model"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9BlBZLeSh2f"
      },
      "source": [
        "We wish to train the neural network in order to find $\\theta$ such that $Q_\\theta(s,a)$ approximates $Q^*(s,a)$. As we discussed in the class, we can use observations of form $(s_i, a_i, r_i, s'_{i})$ (i.e., observing reward $r_i$ and new state $s'_{i}$ on taking action $a_i$ in state $s_i$) for training. Based on observations, we can use stochastic gradient descent to update $\\theta$ in the direction that minimizes the loss function. Further, based on values $Q_\\theta(s,a)$, we can choose the action based on $\\epsilon$-greedy policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHaI6w4BSc3Q"
      },
      "source": [
        "Formally let $d_i$ be the target for the $i$-th sample $(s_t,a_t,r_t,s_{t+1})$\n",
        "\n",
        "$$d_i =  r_t +   \\gamma \\max_a Q_\\theta(s_{t+1},a)$$\n",
        "\n",
        "We can collect a batch of $N$ samples (this generalizes the per sample update $N=1$ discusssed in class), consider the loss fucntion,\n",
        "\n",
        "$$J:=\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - d_i)^2$$\n",
        "\n",
        "and update\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta -\\alpha \\nabla_\\theta J\n",
        "$$\n",
        "\n",
        "This procedure has been shown to be fairly unstable. In class, we discussed two techniques to stabilize the training process: target network and replay buffer.\n",
        "\n",
        "**Replay Buffer**\n",
        "Maintain a buffer $R$ to store trainsition tuples $(s_t,a_t,r_t,s_{t+1})$. When minimizing the loss, we sample batches from the replay buffer and compute gradients for update on these batches. In particular, in each update, we sample $N$ tuples $(s_t,a_t,r_t,s_{t+1})$ from buffer $R$ and then minimize the\n",
        "loss \n",
        "\n",
        "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) -  (r_i + \\gamma \\max_a Q_\\theta(s_i^\\prime,a))^2$$\n",
        "\n",
        "and update parameters.\n",
        "\n",
        "**Target Network**\n",
        "Maintain a target network in addition to the original principal network. The target network is just a copy of the original network but the parameters are not updated by gradients. The target network $\\theta^{-}$ is copied from the principal network every $\\tau$ time steps. Target network is used to compute the targets for update\n",
        "\n",
        "$$d_i = \\max_a r_t + \\gamma Q_{\\theta^{-}}(s_{i}^\\prime,a)$$\n",
        "\n",
        "the targets are used in the loss function to update the principal network parameters. This slowly updated target network ensures that the targets come from a relatively stationary distribution and hence stabilize learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKt2Mt6TTZsk"
      },
      "source": [
        "batch_size = 24\n",
        "# Model used for selecting actions (principal)\n",
        "model = model_creator()\n",
        "# Then create the target model. This will periodically be copied from the principal network \n",
        "model_target = model_creator()\n",
        "\n",
        "model.build((batch_size,obssize,))\n",
        "model_target.build((batch_size,obssize,))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JA7a0AXiVO7"
      },
      "source": [
        "- Complete the code below to learn an Agent using DQN. \n",
        "- You should tweak the Neural network appropriately to achieve a good reward (>100). Ideally you would want to have a reward close to 200.\n",
        "- The reference paper performs updates every 4 actions. You can experiment with this parameter to speed up the learning\n",
        "- You can experiment with other parameters as well, like learning rate, memory size, different exploration schemes (e.g., adaptive $\\epsilon$ or strategic explorations with bonus rewards) and others.\n",
        "\n",
        "- As we mentioned in class, there are multiple ways to improve the efficiency even further. OPTIONALLY you can experiment with these:\n",
        "  - Prioritized Replay buffer.\n",
        "  - Double DQN \n",
        "  -Dueling DQN architecture.\n",
        "\n",
        "- In case you need to debug your code you can try printing relevant information as the training happens. For example although the performance might vary from iteration to iteration, the average Q values might increase overtime in a more smooth way. This is discussed in the refernece paper\n",
        "\n",
        "- Create a plot of the running reward sampled throughout the training at the frequency of your choice at the end of the training\n",
        "- OPTIONALLY you can create a plot for the average Q-values of the principal Q-network sampled at the frequency of your choice\n",
        "\n",
        "- Ideally you want to learn with as few episodes as possible. However you will not be graded on sample efficiency in this homework. You encouraged to try to learn efficiently though.\n",
        "\n",
        "- Note that the skeleton code includes the GradientTape construct to do the learning. Take a look [here](https://www.tensorflow.org/api_docs/python/tf/GradientTape) for an explanation of GradientTape. It allows for more flexibility than model.fit. Also it uses Adam (Adaptive Moment Estimation) for Stochastic Gradient Descent optimizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn81y4Iz_QlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce5a42b-5d13-45a2-c355-6ebfe2f6ac64"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Our Experience Replay memory \n",
        "action_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "\n",
        "# Replay memory size\n",
        "max_memory = 2500 # You can experiment with different sizes.\n",
        "\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "timestep_count = 0\n",
        "\n",
        "\n",
        "# how often to train your model - this allows you to speed up learning\n",
        "# by not performing in every iteration learning. See also refernece paper\n",
        "# you can set this value to other values like 1 as well to learn every time \n",
        "\n",
        "update_after_actions = 25\n",
        "\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.999\n",
        "\n",
        "# How often to update the target network\n",
        "target_update_every = 1000\n",
        "#loss_function = keras.losses.Huber() # You can use the Huber loss function or the mean squared error function \n",
        "loss_function = keras.losses.MeanSquaredError()\n",
        "max_steps_per_episode = 1000\n",
        "\n",
        "max_episodes = 2500\n",
        "# max_steps_per_episode = 1000\n",
        "last_n_reward = 100\n",
        "\n",
        "for episode in range(max_episodes):\n",
        "    state = np.array(env.reset())\n",
        "    episode_reward = 0\n",
        "\n",
        "    epsilon = max(epsilon*epsilon_decay, epsilon_min)\n",
        "\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        timestep_count += 1\n",
        "\n",
        "        # exploration\n",
        "        if np.random.rand() < epsilon:\n",
        "            # Take random action\n",
        "            action = np.random.choice(actsize)\n",
        "        else:\n",
        "            # Predict action Q-values\n",
        "            # From environment state\n",
        "            state_t = tf.convert_to_tensor(state)\n",
        "            state_t = tf.expand_dims(state_t, 0)\n",
        "            action_vals = model(state_t, training=False)\n",
        "            # Choose the best action\n",
        "            action = np.argmax(action_vals[0])\n",
        "\n",
        "        # follow selected action\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        #state_next = np.array(state_next)\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Save action/states and other information in replay buffer\n",
        "        action_history.append(action)\n",
        "        state_history.append(state)\n",
        "        state_next_history.append(state_next)\n",
        "        rewards_history.append(reward)\n",
        "        done_history.append(done)\n",
        "\n",
        "        state = state_next\n",
        "\n",
        "        # Update every Xth frame to speed up (optional)\n",
        "        # and if you have sufficient history\n",
        "        if timestep_count % update_after_actions == 0 and len(action_history) > batch_size:\n",
        "\n",
        "            #  Sample a set of batch_size memories from the history\n",
        "\n",
        "            indexes = np.random.randint(low=0, high=len(action_history) - 1, size=batch_size)\n",
        "            state_sample = tf.convert_to_tensor([state_history[x] for x in indexes])\n",
        "            state_next_sample = tf.convert_to_tensor([state_next_history[x] for x in indexes])\n",
        "            rewards_sample = [rewards_history[x] for x in indexes]\n",
        "            action_sample = [action_history[x] for x in indexes]\n",
        "            done_sample = tf.cast(tf.convert_to_tensor([done_history[x] for x in indexes]), tf.float32)\n",
        "\n",
        "\n",
        "            # Create for the sample states the targets (r+gamma * max Q(...) )\n",
        "            Q_next_state = tf.math.reduce_max(model_target(state_next_sample, training=False), axis=1)\n",
        "\n",
        "            # If the episode was ended (done_sample value is 1)\n",
        "            # you can penalize the Q value of the target by some value `penalty`\n",
        "            # Q_targets = Q_targets * (1 - done_sample) - penalty*done_sample\n",
        "            Q_targets = rewards_sample + gamma * Q_next_state * (1 - done_sample)\n",
        "\n",
        "\n",
        "            # What actions are relevant and need updating\n",
        "            relevant_actions = tf.one_hot(action_sample, actsize)\n",
        "            # we will use Gradient tape to do a custom gradient \n",
        "            # in the `with` environment we will record a set of operations\n",
        "            # and then we will take gradients with respect to the trainable parameters\n",
        "            # in the neural network\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Train the model on your action selecting network\n",
        "                q_values = model(state_sample, training=False)\n",
        "                # We consider only the relevant actions\n",
        "                Q_of_actions = tf.reduce_sum(tf.multiply(q_values, relevant_actions), axis=1)\n",
        "                # Calculate loss between principal network and target network\n",
        "                loss = loss_function(Q_targets, Q_of_actions)\n",
        "\n",
        "            # Nudge the weights of the trainable variables towards \n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if timestep_count % target_update_every == 0:\n",
        "            # update the the target network with new weights\n",
        "            model_target.set_weights(model.get_weights())\n",
        "            # Log details\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}, epsilon {}\"\n",
        "            print(template.format(running_reward, episode_count, timestep_count, epsilon))\n",
        "\n",
        "        # Don't let the memory grow beyond the limit\n",
        "        if len(rewards_history) > max_memory:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del done_history[:1]\n",
        "        if done: break\n",
        "\n",
        "    # reward of last 100\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if len(episode_reward_history) > last_n_reward: del episode_reward_history[:1]\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "    episode_count += 1\n",
        "\n",
        "    # If you want to stop your training once you achieve the reward you want you can\n",
        "    # have an if statement here. Alternatively you can stop after a fixed number\n",
        "    # of episodes."
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 22.64 at episode 42, frame count 1000, epsilon 0.9578907814534664\n",
            "running reward: 22.35 at episode 89, frame count 2000, epsilon 0.9138900318559524\n",
            "running reward: 22.27 at episode 134, frame count 3000, epsilon 0.8736568985103146\n",
            "running reward: 20.03 at episode 189, frame count 4000, epsilon 0.8268805241487632\n",
            "running reward: 18.34 at episode 242, frame count 5000, epsilon 0.784176167005256\n",
            "running reward: 18.09 at episode 298, frame count 6000, epsilon 0.7414484806367364\n",
            "running reward: 16.70 at episode 359, frame count 7000, epsilon 0.6975506718651011\n",
            "running reward: 15.07 at episode 426, frame count 8000, epsilon 0.6523241732116479\n",
            "running reward: 13.80 at episode 498, frame count 9000, epsilon 0.6069859307919768\n",
            "running reward: 14.88 at episode 556, frame count 10000, epsilon 0.572765620160788\n",
            "running reward: 17.58 at episode 615, frame count 11000, epsilon 0.539934088348504\n",
            "running reward: 17.68 at episode 668, frame count 12000, epsilon 0.5120491189128954\n",
            "running reward: 18.08 at episode 724, frame count 13000, epsilon 0.4841489160264175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 17.70 at episode 775, frame count 14000, epsilon 0.4600646486360102\n",
            "running reward: 23.98 at episode 798, frame count 15000, epsilon 0.44959874735743227\n",
            "running reward: 29.64 at episode 826, frame count 16000, epsilon 0.43717846703394564\n",
            "running reward: 33.38 at episode 858, frame count 17000, epsilon 0.4234034438366067\n",
            "running reward: 39.21 at episode 877, frame count 18000, epsilon 0.41543077175087\n",
            "running reward: 34.95 at episode 917, frame count 19000, epsilon 0.39913351012122433\n",
            "running reward: 34.69 at episode 944, frame count 20000, epsilon 0.38849584071717547\n",
            "running reward: 36.11 at episode 966, frame count 21000, epsilon 0.380038079308654\n",
            "running reward: 37.25 at episode 989, frame count 22000, epsilon 0.3713926834236692\n",
            "running reward: 40.72 at episode 1015, frame count 23000, epsilon 0.36185621618376534\n",
            "running reward: 42.83 at episode 1035, frame count 24000, epsilon 0.3546874337726758\n",
            "running reward: 46.36 at episode 1053, frame count 25000, epsilon 0.3483570387995002\n",
            "running reward: 40.51 at episode 1087, frame count 26000, epsilon 0.33670625926950376\n",
            "running reward: 40.02 at episode 1115, frame count 27000, epsilon 0.3274046627873518\n",
            "running reward: 38.57 at episode 1138, frame count 28000, epsilon 0.3199566119774545\n",
            "running reward: 37.44 at episode 1158, frame count 29000, epsilon 0.31361790828888503\n",
            "running reward: 44.56 at episode 1166, frame count 30000, epsilon 0.31111772878333877\n",
            "running reward: 51.47 at episode 1178, frame count 31000, epsilon 0.3074047815158953\n",
            "running reward: 54.63 at episode 1195, frame count 32000, epsilon 0.3022204989748846\n",
            "running reward: 62.39 at episode 1205, frame count 33000, epsilon 0.29921185770452\n",
            "running reward: 70.22 at episode 1214, frame count 34000, epsilon 0.2965296975159237\n",
            "running reward: 74.94 at episode 1228, frame count 35000, epsilon 0.29240515831259833\n",
            "running reward: 78.04 at episode 1240, frame count 36000, epsilon 0.28891553096867023\n",
            "running reward: 81.28 at episode 1257, frame count 37000, epsilon 0.2840430636776882\n",
            "running reward: 79.20 at episode 1267, frame count 38000, epsilon 0.2812153809531867\n",
            "running reward: 80.82 at episode 1275, frame count 39000, epsilon 0.2789735162078359\n",
            "running reward: 87.53 at episode 1284, frame count 40000, epsilon 0.2764727742098891\n",
            "running reward: 93.08 at episode 1288, frame count 41000, epsilon 0.2753685408440802\n",
            "running reward: 99.81 at episode 1295, frame count 42000, epsilon 0.2734467341692626\n",
            "running reward: 99.67 at episode 1305, frame count 43000, epsilon 0.2707245391743544\n",
            "running reward: 87.39 at episode 1331, frame count 44000, epsilon 0.2637729867768368\n",
            "running reward: 90.17 at episode 1340, frame count 45000, epsilon 0.2614085035996406\n",
            "running reward: 92.36 at episode 1350, frame count 46000, epsilon 0.2588061506321157\n",
            "running reward: 97.64 at episode 1357, frame count 47000, epsilon 0.2569999334576916\n",
            "running reward: 103.77 at episode 1364, frame count 48000, epsilon 0.25520632193608234\n",
            "running reward: 104.32 at episode 1371, frame count 49000, epsilon 0.253425228091996\n",
            "running reward: 106.61 at episode 1378, frame count 50000, epsilon 0.25165656456412355\n",
            "running reward: 104.55 at episode 1385, frame count 51000, epsilon 0.24990024460085344\n",
            "running reward: 106.06 at episode 1390, frame count 52000, epsilon 0.248653239882542\n",
            "running reward: 106.84 at episode 1397, frame count 53000, epsilon 0.24691788022723601\n",
            "running reward: 110.71 at episode 1404, frame count 54000, epsilon 0.24519463170764128\n",
            "running reward: 112.35 at episode 1409, frame count 55000, epsilon 0.24397110804469957\n",
            "running reward: 123.26 at episode 1416, frame count 56000, epsilon 0.24226842515120073\n",
            "running reward: 132.17 at episode 1422, frame count 57000, epsilon 0.24081844378493483\n",
            "running reward: 141.27 at episode 1428, frame count 58000, epsilon 0.23937714058612394\n",
            "running reward: 146.47 at episode 1434, frame count 59000, epsilon 0.2379444636157624\n",
            "running reward: 148.55 at episode 1442, frame count 60000, epsilon 0.23604755704357036\n",
            "running reward: 148.43 at episode 1450, frame count 61000, epsilon 0.2341657727166659\n",
            "running reward: 152.34 at episode 1456, frame count 62000, epsilon 0.23276428588715223\n",
            "running reward: 153.64 at episode 1460, frame count 63000, epsilon 0.23183462439849456\n",
            "running reward: 159.18 at episode 1464, frame count 64000, epsilon 0.2309086759815403\n",
            "running reward: 165.46 at episode 1468, frame count 65000, epsilon 0.22998642580626621\n",
            "running reward: 163.04 at episode 1476, frame count 66000, epsilon 0.22815296115658504\n",
            "running reward: 166.22 at episode 1483, frame count 67000, epsilon 0.22656067366330018\n",
            "running reward: 158.53 at episode 1491, frame count 68000, epsilon 0.2247545193013052\n",
            "running reward: 160.17 at episode 1497, frame count 69000, epsilon 0.22340935901156642\n",
            "running reward: 160.75 at episode 1503, frame count 70000, epsilon 0.22207224953304477\n",
            "running reward: 163.85 at episode 1508, frame count 71000, epsilon 0.2209641067882625\n",
            "running reward: 164.34 at episode 1513, frame count 72000, epsilon 0.2198614936868526\n",
            "running reward: 165.95 at episode 1518, frame count 73000, epsilon 0.21876438263583933\n",
            "running reward: 162.06 at episode 1526, frame count 74000, epsilon 0.21702038074196223\n",
            "running reward: 161.63 at episode 1534, frame count 75000, epsilon 0.21529028212872525\n",
            "running reward: 161.71 at episode 1541, frame count 76000, epsilon 0.21378776372211966\n",
            "running reward: 154.66 at episode 1552, frame count 77000, epsilon 0.21144782144365132\n",
            "running reward: 149.98 at episode 1558, frame count 78000, epsilon 0.21018230200652513\n",
            "running reward: 136.99 at episode 1569, frame count 79000, epsilon 0.207881822100247\n",
            "running reward: 138.01 at episode 1577, frame count 80000, epsilon 0.20622457658762192\n",
            "running reward: 126.64 at episode 1593, frame count 81000, epsilon 0.2029496152000772\n",
            "running reward: 122.18 at episode 1602, frame count 82000, epsilon 0.20113035782720212\n",
            "running reward: 118.44 at episode 1607, frame count 83000, epsilon 0.20012671533134627\n",
            "running reward: 116.73 at episode 1615, frame count 84000, epsilon 0.1985312939636264\n",
            "running reward: 113.38 at episode 1625, frame count 85000, epsilon 0.1965548911501048\n",
            "running reward: 109.64 at episode 1634, frame count 86000, epsilon 0.19479295661996557\n",
            "running reward: 84.68 at episode 1664, frame count 87000, epsilon 0.18903311730874942\n",
            "running reward: 75.46 at episode 1683, frame count 88000, epsilon 0.18547363030035172\n",
            "running reward: 78.50 at episode 1694, frame count 89000, epsilon 0.18344359087468604\n",
            "running reward: 82.12 at episode 1700, frame count 90000, epsilon 0.18234567731717977\n",
            "running reward: 86.46 at episode 1705, frame count 91000, epsilon 0.1814357705648218\n",
            "running reward: 84.81 at episode 1710, frame count 92000, epsilon 0.1805304042562526\n",
            "running reward: 88.32 at episode 1717, frame count 93000, epsilon 0.17927047625269885\n",
            "running reward: 90.76 at episode 1724, frame count 94000, epsilon 0.17801934133073533\n",
            "running reward: 95.49 at episode 1729, frame count 95000, epsilon 0.17713102303819148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 98.61 at episode 1735, frame count 96000, epsilon 0.17607089032534334\n",
            "running reward: 104.42 at episode 1743, frame count 97000, epsilon 0.17466724334001496\n",
            "running reward: 112.40 at episode 1751, frame count 98000, epsilon 0.17327478630695967\n",
            "running reward: 120.97 at episode 1757, frame count 99000, epsilon 0.1722377332480149\n",
            "running reward: 130.10 at episode 1762, frame count 100000, epsilon 0.17137826523759098\n",
            "running reward: 137.44 at episode 1769, frame count 101000, epsilon 0.17018221033225317\n",
            "running reward: 144.65 at episode 1776, frame count 102000, epsilon 0.16899450273591984\n",
            "running reward: 149.44 at episode 1783, frame count 103000, epsilon 0.16781508419242955\n",
            "running reward: 152.11 at episode 1789, frame count 104000, epsilon 0.1668107075597524\n",
            "running reward: 158.66 at episode 1796, frame count 105000, epsilon 0.16564652979915298\n",
            "running reward: 158.74 at episode 1800, frame count 106000, epsilon 0.1649849368967147\n",
            "running reward: 159.84 at episode 1804, frame count 107000, epsilon 0.16432598639897444\n",
            "running reward: 158.25 at episode 1811, frame count 108000, epsilon 0.16317914959423443\n",
            "running reward: 160.88 at episode 1815, frame count 109000, epsilon 0.16252741141820165\n",
            "running reward: 164.97 at episode 1820, frame count 110000, epsilon 0.16171639801076318\n",
            "running reward: 166.58 at episode 1825, frame count 111000, epsilon 0.16090943156833393\n",
            "running reward: 163.44 at episode 1832, frame count 112000, epsilon 0.15978643901921685\n",
            "running reward: 166.79 at episode 1838, frame count 113000, epsilon 0.1588301139883539\n",
            "running reward: 166.76 at episode 1845, frame count 114000, epsilon 0.1577216330693309\n",
            "running reward: 169.19 at episode 1851, frame count 115000, epsilon 0.1567776659433432\n",
            "running reward: 167.50 at episode 1859, frame count 116000, epsilon 0.15552782562185924\n",
            "running reward: 165.64 at episode 1865, frame count 117000, epsilon 0.15459698847728787\n",
            "running reward: 165.37 at episode 1872, frame count 118000, epsilon 0.15351805068921792\n",
            "running reward: 168.32 at episode 1878, frame count 119000, epsilon 0.15259924208778378\n",
            "running reward: 170.29 at episode 1883, frame count 120000, epsilon 0.15183777034453613\n",
            "running reward: 172.81 at episode 1887, frame count 121000, epsilon 0.15123132968258082\n",
            "running reward: 177.11 at episode 1891, frame count 122000, epsilon 0.15062731114705447\n",
            "running reward: 181.71 at episode 1895, frame count 123000, epsilon 0.15002570506397447\n",
            "running reward: 181.69 at episode 1899, frame count 124000, epsilon 0.14942650179799613\n",
            "running reward: 184.14 at episode 1903, frame count 125000, epsilon 0.14882969175225835\n",
            "running reward: 185.42 at episode 1906, frame count 126000, epsilon 0.14838364901724713\n",
            "running reward: 189.52 at episode 1911, frame count 127000, epsilon 0.14764321312555637\n",
            "running reward: 190.97 at episode 1915, frame count 128000, epsilon 0.1470535255419077\n",
            "running reward: 187.17 at episode 1921, frame count 129000, epsilon 0.1461734072526738\n",
            "running reward: 187.27 at episode 1927, frame count 130000, epsilon 0.14529855648899012\n",
            "running reward: 193.27 at episode 1931, frame count 131000, epsilon 0.1447182334733242\n",
            "running reward: 196.61 at episode 1934, frame count 132000, epsilon 0.1442845127828864\n",
            "running reward: 200.28 at episode 1938, frame count 133000, epsilon 0.14370823986183776\n",
            "running reward: 203.19 at episode 1942, frame count 134000, epsilon 0.14313426857714032\n",
            "running reward: 206.88 at episode 1945, frame count 135000, epsilon 0.14270529503108034\n",
            "running reward: 210.87 at episode 1951, frame count 136000, epsilon 0.14185120098835316\n",
            "running reward: 213.18 at episode 1955, frame count 137000, epsilon 0.14128464672434274\n",
            "running reward: 214.94 at episode 1961, frame count 138000, epsilon 0.14043905529012304\n",
            "running reward: 216.72 at episode 1967, frame count 139000, epsilon 0.13959852473753628\n",
            "running reward: 217.74 at episode 1973, frame count 140000, epsilon 0.13876302477710473\n",
            "running reward: 218.29 at episode 1979, frame count 141000, epsilon 0.13793252530063388\n",
            "running reward: 218.37 at episode 1984, frame count 142000, epsilon 0.13724424062074797\n",
            "running reward: 212.44 at episode 1990, frame count 143000, epsilon 0.1364228310978058\n",
            "running reward: 205.86 at episode 1997, frame count 144000, epsilon 0.1354707313895471\n",
            "running reward: 195.27 at episode 2005, frame count 145000, epsilon 0.13439075114202403\n",
            "running reward: 190.78 at episode 2011, frame count 146000, epsilon 0.13358641981063907\n",
            "running reward: 186.26 at episode 2018, frame count 147000, epsilon 0.13265411551592862\n",
            "running reward: 187.35 at episode 2023, frame count 148000, epsilon 0.13199217015362613\n",
            "running reward: 189.07 at episode 2028, frame count 149000, epsilon 0.13133352790529762\n",
            "running reward: 179.85 at episode 2034, frame count 150000, epsilon 0.13054749411608307\n",
            "running reward: 174.02 at episode 2040, frame count 151000, epsilon 0.12976616475480587\n",
            "running reward: 166.13 at episode 2048, frame count 152000, epsilon 0.12873166163155172\n",
            "running reward: 161.50 at episode 2055, frame count 153000, epsilon 0.1278332388639199\n",
            "running reward: 157.61 at episode 2063, frame count 154000, epsilon 0.1268141451339765\n",
            "running reward: 153.44 at episode 2071, frame count 155000, epsilon 0.12580317567624627\n",
            "running reward: 149.91 at episode 2079, frame count 156000, epsilon 0.12480026572357655\n",
            "running reward: 143.47 at episode 2088, frame count 157000, epsilon 0.1236815456741172\n",
            "running reward: 142.59 at episode 2095, frame count 158000, epsilon 0.12281836784232969\n",
            "running reward: 144.57 at episode 2101, frame count 159000, epsilon 0.12208329745626752\n",
            "running reward: 145.95 at episode 2107, frame count 160000, epsilon 0.12135262648115634\n",
            "running reward: 145.53 at episode 2114, frame count 161000, epsilon 0.12050570225784722\n",
            "running reward: 145.53 at episode 2120, frame count 162000, epsilon 0.11978447322152684\n",
            "running reward: 142.29 at episode 2127, frame count 163000, epsilon 0.11894849319464719\n",
            "running reward: 141.56 at episode 2133, frame count 164000, epsilon 0.11823658408569088\n",
            "running reward: 140.58 at episode 2140, frame count 165000, epsilon 0.11741140683121218\n",
            "running reward: 141.80 at episode 2146, frame count 166000, epsilon 0.1167086972148597\n",
            "running reward: 143.35 at episode 2153, frame count 167000, epsilon 0.11589418313627514\n",
            "running reward: 144.76 at episode 2156, frame count 168000, epsilon 0.11554684815352155\n",
            "running reward: 149.79 at episode 2163, frame count 169000, epsilon 0.11474044266016015\n",
            "running reward: 153.34 at episode 2167, frame count 170000, epsilon 0.11428216887332844\n",
            "running reward: 158.74 at episode 2171, frame count 171000, epsilon 0.11382572543383399\n",
            "running reward: 163.90 at episode 2176, frame count 172000, epsilon 0.11325773392623092\n",
            "running reward: 169.19 at episode 2179, frame count 173000, epsilon 0.11291830038439628\n",
            "running reward: 171.74 at episode 2182, frame count 174000, epsilon 0.11257988412522593\n",
            "running reward: 181.73 at episode 2185, frame count 175000, epsilon 0.11224248209992274\n",
            "running reward: 187.18 at episode 2189, frame count 176000, epsilon 0.11179418517755796\n",
            "running reward: 192.63 at episode 2194, frame count 177000, epsilon 0.11123633107613895\n",
            "running reward: 190.64 at episode 2200, frame count 178000, epsilon 0.11057057941158951\n",
            "running reward: 191.27 at episode 2207, frame count 179000, epsilon 0.10979890347177342\n",
            "running reward: 191.98 at episode 2212, frame count 180000, epsilon 0.10925100584600911\n",
            "running reward: 190.78 at episode 2220, frame count 181000, epsilon 0.10838005071698985\n",
            "running reward: 187.72 at episode 2229, frame count 182000, epsilon 0.10740852285208072\n",
            "running reward: 182.06 at episode 2238, frame count 183000, epsilon 0.10644570384443867\n",
            "running reward: 177.12 at episode 2248, frame count 184000, epsilon 0.10538602411150963\n",
            "running reward: 169.47 at episode 2257, frame count 185000, epsilon 0.1044413349522134\n",
            "running reward: 164.27 at episode 2265, frame count 186000, epsilon 0.10360872278856463\n",
            "running reward: 155.84 at episode 2273, frame count 187000, epsilon 0.10278274825565252\n",
            "running reward: 146.67 at episode 2281, frame count 188000, epsilon 0.1019633584379136\n",
            "running reward: 133.06 at episode 2288, frame count 189000, epsilon 0.10125175259422443\n",
            "running reward: 126.67 at episode 2296, frame count 190000, epsilon 0.1004445679595271\n",
            "running reward: 125.10 at episode 2304, frame count 191000, epsilon 0.09964381824588342\n",
            "running reward: 121.63 at episode 2312, frame count 192000, epsilon 0.09884945215374291\n",
            "running reward: 119.79 at episode 2319, frame count 193000, epsilon 0.09815957837088878\n",
            "running reward: 122.72 at episode 2326, frame count 194000, epsilon 0.09747451924128663\n",
            "running reward: 125.01 at episode 2333, frame count 195000, epsilon 0.09679424116330308\n",
            "running reward: 128.18 at episode 2340, frame count 196000, epsilon 0.09611871076981171\n",
            "running reward: 130.76 at episode 2347, frame count 197000, epsilon 0.09544789492655645\n",
            "running reward: 133.71 at episode 2353, frame count 198000, epsilon 0.09487663736789426\n",
            "running reward: 137.15 at episode 2359, frame count 199000, epsilon 0.09430879879713726\n",
            "running reward: 139.57 at episode 2366, frame count 200000, epsilon 0.09365061439282289\n",
            "running reward: 140.88 at episode 2372, frame count 201000, epsilon 0.09309011359407375\n",
            "running reward: 141.71 at episode 2379, frame count 202000, epsilon 0.09244043443640296\n",
            "running reward: 142.25 at episode 2386, frame count 203000, epsilon 0.09179528941228957\n",
            "running reward: 143.09 at episode 2394, frame count 204000, epsilon 0.09106349223097913\n",
            "running reward: 143.90 at episode 2400, frame count 205000, epsilon 0.09051847541007228\n",
            "running reward: 146.88 at episode 2406, frame count 206000, epsilon 0.08997672052573072\n",
            "running reward: 148.03 at episode 2413, frame count 207000, epsilon 0.08934876984714372\n",
            "running reward: 148.91 at episode 2420, frame count 208000, epsilon 0.0887252016582989\n",
            "running reward: 147.92 at episode 2428, frame count 209000, epsilon 0.08801787938827345\n",
            "running reward: 147.22 at episode 2435, frame count 210000, epsilon 0.08740359953047568\n",
            "running reward: 148.87 at episode 2441, frame count 211000, epsilon 0.08688048724052432\n",
            "running reward: 152.08 at episode 2446, frame count 212000, epsilon 0.08644695274082355\n",
            "running reward: 154.26 at episode 2450, frame count 213000, epsilon 0.08610168326587533\n",
            "running reward: 159.30 at episode 2454, frame count 214000, epsilon 0.08575779279859079\n",
            "running reward: 160.62 at episode 2458, frame count 215000, epsilon 0.08541527583120778\n",
            "running reward: 162.03 at episode 2464, frame count 216000, epsilon 0.08490406369833321\n",
            "running reward: 160.97 at episode 2472, frame count 217000, epsilon 0.08422720375384107\n",
            "running reward: 162.38 at episode 2478, frame count 218000, epsilon 0.08372310225609315\n",
            "running reward: 161.50 at episode 2485, frame count 219000, epsilon 0.08313879579806785\n",
            "running reward: 163.96 at episode 2491, frame count 220000, epsilon 0.0826412084436871\n",
            "running reward: 165.29 at episode 2496, frame count 221000, epsilon 0.08222882798755414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-kUmOPTiMXX"
      },
      "source": [
        "Evaluate the performance of the agent on 100 episodes on the environment and print out the average testing performance. Alternatively you can make sure the code above terminates with 100 episodes where there is no exploration at all (epsilon=0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe2V09jdiNXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2fae82-939e-417b-a95e-cd9563d1e2f2"
      },
      "source": [
        "# YOUR CODE Here\n",
        "test_episodes = 100\n",
        "\n",
        "avg_reward = 0\n",
        "\n",
        "for episode in range(test_episodes):\n",
        "    state = np.array(env.reset())\n",
        "\n",
        "    episode_reward = 0\n",
        "\n",
        "    for timestep in range(1, 1000):\n",
        "        state = tf.convert_to_tensor(state)\n",
        "        state = tf.expand_dims(state, 0)\n",
        "        action_vals = model(state, training=False)\n",
        "        action = np.argmax(action_vals[0])\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "\n",
        "        episode_reward += reward\n",
        "        avg_reward += reward\n",
        "\n",
        "        state = state_next\n",
        "\n",
        "        if done:\n",
        "            template = \"running reward: {:.2f} at episode {}\"\n",
        "            print(template.format(episode_reward, episode))\n",
        "            break\n",
        "template = \"average reward: {:.2f} after {} episodes\"\n",
        "print(template.format(avg_reward/test_episodes, test_episodes))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 224.00 at episode 0\n",
            "running reward: 244.00 at episode 1\n",
            "running reward: 239.00 at episode 2\n",
            "running reward: 187.00 at episode 3\n",
            "running reward: 196.00 at episode 4\n",
            "running reward: 194.00 at episode 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 209.00 at episode 6\n",
            "running reward: 217.00 at episode 7\n",
            "running reward: 206.00 at episode 8\n",
            "running reward: 235.00 at episode 9\n",
            "running reward: 213.00 at episode 10\n",
            "running reward: 201.00 at episode 11\n",
            "running reward: 252.00 at episode 12\n",
            "running reward: 237.00 at episode 13\n",
            "running reward: 235.00 at episode 14\n",
            "running reward: 219.00 at episode 15\n",
            "running reward: 254.00 at episode 16\n",
            "running reward: 262.00 at episode 17\n",
            "running reward: 209.00 at episode 18\n",
            "running reward: 222.00 at episode 19\n",
            "running reward: 241.00 at episode 20\n",
            "running reward: 191.00 at episode 21\n",
            "running reward: 224.00 at episode 22\n",
            "running reward: 217.00 at episode 23\n",
            "running reward: 215.00 at episode 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 224.00 at episode 25\n",
            "running reward: 252.00 at episode 26\n",
            "running reward: 191.00 at episode 27\n",
            "running reward: 226.00 at episode 28\n",
            "running reward: 214.00 at episode 29\n",
            "running reward: 231.00 at episode 30\n",
            "running reward: 196.00 at episode 31\n",
            "running reward: 205.00 at episode 32\n",
            "running reward: 241.00 at episode 33\n",
            "running reward: 209.00 at episode 34\n",
            "running reward: 203.00 at episode 35\n",
            "running reward: 183.00 at episode 36\n",
            "running reward: 226.00 at episode 37\n",
            "running reward: 198.00 at episode 38\n",
            "running reward: 192.00 at episode 39\n",
            "running reward: 200.00 at episode 40\n",
            "running reward: 203.00 at episode 41\n",
            "running reward: 200.00 at episode 42\n",
            "running reward: 212.00 at episode 43\n",
            "running reward: 229.00 at episode 44\n",
            "running reward: 214.00 at episode 45\n",
            "running reward: 220.00 at episode 46\n",
            "running reward: 240.00 at episode 47\n",
            "running reward: 212.00 at episode 48\n",
            "running reward: 199.00 at episode 49\n",
            "running reward: 239.00 at episode 50\n",
            "running reward: 225.00 at episode 51\n",
            "running reward: 211.00 at episode 52\n",
            "running reward: 184.00 at episode 53\n",
            "running reward: 184.00 at episode 54\n",
            "running reward: 230.00 at episode 55\n",
            "running reward: 254.00 at episode 56\n",
            "running reward: 199.00 at episode 57\n",
            "running reward: 179.00 at episode 58\n",
            "running reward: 207.00 at episode 59\n",
            "running reward: 246.00 at episode 60\n",
            "running reward: 214.00 at episode 61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 200.00 at episode 62\n",
            "running reward: 231.00 at episode 63\n",
            "running reward: 241.00 at episode 64\n",
            "running reward: 226.00 at episode 65\n",
            "running reward: 234.00 at episode 66\n",
            "running reward: 201.00 at episode 67\n",
            "running reward: 238.00 at episode 68\n",
            "running reward: 249.00 at episode 69\n",
            "running reward: 244.00 at episode 70\n",
            "running reward: 237.00 at episode 71\n",
            "running reward: 236.00 at episode 72\n",
            "running reward: 207.00 at episode 73\n",
            "running reward: 212.00 at episode 74\n",
            "running reward: 187.00 at episode 75\n",
            "running reward: 226.00 at episode 76\n",
            "running reward: 203.00 at episode 77\n",
            "running reward: 197.00 at episode 78\n",
            "running reward: 224.00 at episode 79\n",
            "running reward: 199.00 at episode 80\n",
            "running reward: 214.00 at episode 81\n",
            "running reward: 227.00 at episode 82\n",
            "running reward: 189.00 at episode 83\n",
            "running reward: 204.00 at episode 84\n",
            "running reward: 208.00 at episode 85\n",
            "running reward: 223.00 at episode 86\n",
            "running reward: 196.00 at episode 87\n",
            "running reward: 228.00 at episode 88\n",
            "running reward: 203.00 at episode 89\n",
            "running reward: 218.00 at episode 90\n",
            "running reward: 204.00 at episode 91\n",
            "running reward: 235.00 at episode 92\n",
            "running reward: 211.00 at episode 93\n",
            "running reward: 217.00 at episode 94\n",
            "running reward: 233.00 at episode 95\n",
            "running reward: 212.00 at episode 96\n",
            "running reward: 221.00 at episode 97\n",
            "running reward: 184.00 at episode 98\n",
            "running reward: 230.00 at episode 99\n",
            "average reward: 216.84 after 100 episodes\n"
          ]
        }
      ]
    }
  ]
}